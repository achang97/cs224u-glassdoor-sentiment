{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glassdoor Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aspect-based sentiment analysis (or joint sentiment analysis and topic spotting) is a really interesting avenue that is supported by the Glassdoor data's rating structure. We will utilize feature-specific approaches to predict ratings on workplace culture, benefits, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'cs224u_files')\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import utils\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import helpers\n",
    "import csv\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLASSDOOR_HOME = 'glassdoor-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These splits are all stratified. We create a separate splits for the following:\n",
    "\n",
    "1. rating_overall\n",
    "2. rating_balance\n",
    "3. rating_culture\n",
    "4. rating_career\n",
    "5. rating_comp\n",
    "6. rating_mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created splits for rating_overall\n",
      "Created splits for rating_balance\n",
      "Created splits for rating_culture\n",
      "Created splits for rating_career\n",
      "Created splits for rating_comp\n",
      "Created splits for rating_mgmt\n"
     ]
    }
   ],
   "source": [
    "overall_split = helpers.train_dev_test_split(GLASSDOOR_HOME, random_state=1)\n",
    "balance_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_balance', random_state=1)\n",
    "culture_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_culture', random_state=1)\n",
    "career_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_career', random_state=1)\n",
    "comp_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_comp', random_state=1)\n",
    "mgmt_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_mgmt', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_splits = (\n",
    "#     ('Work/Life Balance', balance_split), \n",
    "    ('Culture & Values', culture_split), \n",
    "#     ('Career Opportunities', career_split), \n",
    "#     ('Compensation and Benefits', comp_split),\n",
    "#     ('Senior Management', mgmt_split),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show that these splits are stratified by checking the distribution. We'll do so for overall_split. As we can see, the overall ratings have a heavy left skew, as most ratings are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_hist(y, title):\n",
    "    ax = y.plot.hist(subplots=True, sharex=True, sharey=True, bins=[1, 2, 3, 4, 5, 6], title=title, xticks=[1, 2, 3, 4, 5])\n",
    "\n",
    "for feature_name, split in feature_splits:\n",
    "    X_train, X_dev, X_test, y_train, y_dev, y_test = split\n",
    "    \n",
    "    plot_hist(y_train, '{} Train Ratings'.format(feature_name))\n",
    "    plot_hist(y_dev, '{} Dev Ratings'.format(feature_name))\n",
    "    plot_hist(y_test, '{} Test Ratings'.format(feature_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Experimentation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(phi, classifier, assess='dev', verbose=True, vectorize=True):\n",
    "    all_results = []\n",
    "    \n",
    "    for feature_name, split in feature_splits:\n",
    "        X_train = split[0]\n",
    "        y_train = split[3]\n",
    "        \n",
    "        if assess == 'dev':\n",
    "            X_assess = split[1]\n",
    "            y_assess = split[4]\n",
    "        else: # use test\n",
    "            X_assess = split[2]\n",
    "            y_assess = split[5]\n",
    "            \n",
    "        if verbose:\n",
    "            print (feature_name)\n",
    "        \n",
    "        experimental_results = helpers.experiment(\n",
    "            phi,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_assess,\n",
    "            y_assess,\n",
    "            classifier,\n",
    "            verbose=verbose,\n",
    "            vectorize=vectorize)\n",
    "        \n",
    "        all_results.append((feature_name, experimental_results))\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ci(vals):\n",
    "    if len(set(vals)) == 1:\n",
    "        return (vals[0], vals[0])\n",
    "    loc = np.mean(vals)\n",
    "    scale = np.std(vals) / np.sqrt(len(vals))\n",
    "    return stats.t.interval(0.95, len(vals)-1, loc=loc, scale=scale)\n",
    "\n",
    "def plot_score(scores, score_func=\"Macro F1\"):\n",
    "    scores = pd.Series(scores)\n",
    "    \n",
    "    ax = scores.plot()\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    _ = ax.set_ylabel(score_func)\n",
    "\n",
    "def illustrative_confusion_matrix(y, predictions, classes, class_func=lambda x: x,):\n",
    "    true_vals = [class_func(curr_y) for curr_y in y.values]\n",
    "    conf_matrix = confusion_matrix(true_vals, predictions, labels=classes)\n",
    "    \n",
    "    ex = pd.DataFrame(\n",
    "        conf_matrix,        \n",
    "        columns=classes,\n",
    "        index=classes)\n",
    "    ex.index.name = \"observed\"\n",
    "    \n",
    "    return ex\n",
    "\n",
    "def accuracy(cm):\n",
    "    return cm.values.diagonal().sum() / cm.values.sum()\n",
    "\n",
    "def precision(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=0)\n",
    "\n",
    "def recall(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "def f_score(cm, beta):\n",
    "    p = precision(cm)\n",
    "    r = recall(cm)\n",
    "    return (beta**2 + 1) * ((p * r) / ((beta**2 * p) + r))\n",
    "\n",
    "def f1_score(cm):\n",
    "    return f_score(cm, beta=1.0)\n",
    "\n",
    "def macro_f_score(cm, beta):\n",
    "    return f_score(cm, beta).mean(skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A softmax baseline\n",
    "\n",
    "We first utilize a baseline approach that leverages a bag of words featurizer and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "def unigrams_phi(example):\n",
    "    counter = Counter()\n",
    "    \n",
    "    parts = example[['review_title', 'pros', 'cons', 'advice_to_mgmt']]\n",
    "    for part in parts:\n",
    "        counter.update(word_tokenize(part))\n",
    "        \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):        \n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr',\n",
    "        class_weight='balanced',\n",
    "        max_iter=200)\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culture & Values\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.419     0.439     0.429       816\n",
      "           2      0.204     0.204     0.204       852\n",
      "           3      0.287     0.282     0.285      1537\n",
      "           4      0.328     0.293     0.309      1792\n",
      "           5      0.598     0.638     0.617      2636\n",
      "\n",
      "   micro avg      0.416     0.416     0.416      7633\n",
      "   macro avg      0.367     0.371     0.369      7633\n",
      "weighted avg      0.409     0.416     0.412      7633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_results = experiment(\n",
    "    unigrams_phi,\n",
    "    fit_softmax_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Review Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "def structured_unigrams_phi(example):\n",
    "    counter = Counter()\n",
    "    \n",
    "    labels = ['review_title', 'pros', 'cons', 'advice_to_mgmt']\n",
    "    parts = example[labels]\n",
    "    for label, part in zip(labels, parts):\n",
    "        counter.update(['{}/{}'.format(label, word) for word in word_tokenize(part)])\n",
    "    \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culture & Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chungchang/anaconda3/envs/nlu/lib/python3.7/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.454     0.468     0.461       816\n",
      "           2      0.246     0.237     0.241       852\n",
      "           3      0.299     0.304     0.301      1537\n",
      "           4      0.326     0.290     0.307      1792\n",
      "           5      0.612     0.653     0.632      2636\n",
      "\n",
      "   micro avg      0.431     0.431     0.431      7633\n",
      "   macro avg      0.387     0.390     0.388      7633\n",
      "weighted avg      0.424     0.431     0.427      7633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "structured_softmax_results = experiment(\n",
    "    structured_unigrams_phi,\n",
    "    fit_softmax_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Vector Representations\n",
    "\n",
    "We will model each section as the sum of its 300-dimensional GloVe representations, resulting in a 1200-dimensional vector for each review. A neural network might do better here, since there might be complex relationships between the input feature dimensions that a linear classifier can't learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HOME = 'cs224u_files/data'\n",
    "GLOVE_HOME = os.path.join(DATA_HOME, 'glove.6B')\n",
    "\n",
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "def vsm_leaves_phi(example, lookup, np_func=np.sum):\n",
    "    labels = ['review_title', 'pros', 'cons', 'advice_to_mgmt']\n",
    "    parts = example[labels]\n",
    "    \n",
    "    dim = len(next(iter(lookup.values())))\n",
    "    feats = np.zeros(dim * 4)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        allvecs = np.array([lookup[w] for w in word_tokenize(parts[i]) if w in lookup])    \n",
    "        if len(allvecs) != 0:\n",
    "            feats[i * dim : (i + 1) * dim] = np_func(allvecs, axis=0)  \n",
    "        \n",
    "    return feats\n",
    "\n",
    "def glove_leaves_phi(example, np_func=np.sum):\n",
    "    return vsm_leaves_phi(example, glove_lookup, np_func=np_func)\n",
    "\n",
    "def fit_torch_shallow_neural_classifier(X, y):\n",
    "    mod = TorchShallowNeuralClassifier(\n",
    "        hidden_dim=100, \n",
    "        hidden_activation=nn.ReLU()\n",
    "    )\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culture & Values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 61.98541522026062"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.550     0.352     0.429       816\n",
      "           2      0.223     0.052     0.084       852\n",
      "           3      0.294     0.341     0.316      1537\n",
      "           4      0.291     0.336     0.312      1792\n",
      "           5      0.574     0.667     0.617      2636\n",
      "\n",
      "   micro avg      0.421     0.421     0.421      7633\n",
      "   macro avg      0.386     0.350     0.352      7633\n",
      "weighted avg      0.410     0.421     0.405      7633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glove_neural_results = experiment(\n",
    "    glove_leaves_phi,\n",
    "    fit_torch_shallow_neural_classifier,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNClassifier wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(example):\n",
    "    parts = example[['review_title', 'pros', 'cons', 'advice_to_mgmt']]  \n",
    "\n",
    "    words = []\n",
    "    for part in parts:\n",
    "        words.extend(word_tokenize(part))\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn_classifier(X, y):   \n",
    "    sst_glove_vocab = utils.get_vocab(X, n_words=10000) \n",
    "    mod = TorchRNNClassifier(\n",
    "        sst_glove_vocab, \n",
    "        eta=0.05,\n",
    "        embedding=None,\n",
    "        batch_size=1000,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_iter=50,\n",
    "        l2_strength=0.001,\n",
    "        bidirectional=True,\n",
    "        hidden_activation=nn.ReLU())\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Culture & Values\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d138f692263c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mrnn_phi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfit_rnn_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     vectorize=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-0ffc1ac62aaf>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(phi, classifier, assess, verbose, vectorize)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             vectorize=vectorize)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/andrew/CS224U/glassdoor-sentiment-analysis/helpers.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(phi, X_train, y_train, X_assess, y_assess, train_func, score_func, vectorize, class_func, verbose, random_state)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;31m# Train:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;31m# Predictions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_assess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-d101064dae45>\u001b[0m in \u001b[0;36mfit_rnn_classifier\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         hidden_activation=nn.ReLU())\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/andrew/CS224U/glassdoor-sentiment-analysis/cs224u_files/torch_rnn_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0;31m# Backprop:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# Incremental predictions where possible:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "softmax_results = experiment(\n",
    "    rnn_phi,\n",
    "    fit_rnn_classifier,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "This section begins to build an error-analysis framework using the dicts returned by `helpers.experiment`. These have the following structure:\n",
    "\n",
    "```\n",
    "'model': trained model\n",
    "'train_dataset':\n",
    "   'X': feature matrix\n",
    "   'y': list of labels\n",
    "   'vectorizer': DictVectorizer,\n",
    "   'raw_examples': list of raw inputs, before featurizing   \n",
    "'assess_dataset': same structure as the value of 'train_dataset'\n",
    "'predictions': predictions on the assessment data\n",
    "'metric': `score_func.__name__`, where `score_func` is an `sst.experiment` argument\n",
    "'score': the `score_func` score on the assessment data\n",
    "```\n",
    "The following function just finds mistakes, and returns a `pd.DataFrame` for easy subsequent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(ex):\n",
    "    review_title, props, cons, advice = ex[['review_title', 'pros', 'cons', 'advice_to_mgmt']]\n",
    "    return 'Title: {}\\nPros:\\n {}\\nCons:\\n{}\\nAdvice to Mgmt:\\n{}'.format(review_title, props, cons, advice)\n",
    "\n",
    "def find_errors(experiment):\n",
    "    \"\"\"Find mistaken predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment : dict\n",
    "        As returned by `sst.experiment`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    raw_examples = experiment['assess_dataset']['raw_examples']\n",
    "    raw_examples = [format_example(ex) for ex in raw_examples]\n",
    "    df = pd.DataFrame({\n",
    "        'raw_examples': raw_examples,\n",
    "        'predicted': experiment['predictions'],\n",
    "        'gold': experiment['assess_dataset']['y']})\n",
    "    df['correct'] = df['predicted'] == df['gold']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_analysis = find_errors(softmax_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_analysis = find_errors(rnn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge the sotmax and RNN experiments into a single DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = softmax_analysis.merge(\n",
    "    rnn_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code collects a specific subset of examples; small modifications to its structure will give you different interesting subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where the softmax model is correct, the RNN is not,\n",
    "# and the gold label is 'positive'\n",
    "\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])    \n",
    "    &\n",
    "    (analysis['gold'] == 'positive')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_group.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "It 's a beautiful madness .\n",
      "======================================================================\n",
      "One of the smartest takes on singles culture I 've seen in a long time .\n",
      "======================================================================\n",
      "But it still jingles in the pocket .\n",
      "======================================================================\n",
      "For the first time in years , De Niro digs deep emotionally , perhaps because he 's been stirred by the powerful work of his co-stars .\n",
      "======================================================================\n",
      "With Rabbit-Proof Fence , Noyce has tailored an epic tale into a lean , economical movie .\n"
     ]
    }
   ],
   "source": [
    "for ex in error_group['raw_examples'].sample(5):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>319</td>\n",
       "      <td>119</td>\n",
       "      <td>181</td>\n",
       "      <td>93</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>124</td>\n",
       "      <td>251</td>\n",
       "      <td>174</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122</td>\n",
       "      <td>143</td>\n",
       "      <td>423</td>\n",
       "      <td>411</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "      <td>323</td>\n",
       "      <td>519</td>\n",
       "      <td>797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>57</td>\n",
       "      <td>239</td>\n",
       "      <td>430</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1    2    3    4     5\n",
       "observed                          \n",
       "1         319  119  181   93   104\n",
       "2         151  124  251  174   152\n",
       "3         122  143  423  411   438\n",
       "4          58   95  323  519   797\n",
       "5          28   57  239  430  1882"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name, softmax_experiment = softmax_results[0]\n",
    "illustrative_confusion_matrix(culture_split[4], softmax_experiment['predictions'], [1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "Your task is to develop an original model for the SST ternary problem. There are many options. If you spend more than a few hours on this homework problem, you should consider letting it grow into your final project! Here are some relatively manageable ideas that you might try:\n",
    "\n",
    "1. We didn't systematically evaluate the `bidirectional` option to the `TorchRNNClassifier`. Similarly, that model could be tweaked to allow multiple LSTM layers (at present there is only one), and you could try adding layers to the classifier portion of the model as well.\n",
    "\n",
    "1. We've already glimpsed the power of rich initial word representations, and later in the course we'll see that smart initialization usually leads to a performance gain in NLP, so you could perhaps achieve a winning entry with a simple model that starts in a great place.\n",
    "\n",
    "1. The [practical introduction to contextual word representations](contextualreps.ipynb) (to be discussed later in the quarter) covers pretrained representations and interfaces that are likely to boost the performance of any system.\n",
    "\n",
    "1. The `TreeNN` and `TorchTreeNN` don't perform all that well, and this could be for the same reason that RNNs don't peform well: the gradient signal doesn't propagate reliably down inside very deep trees. [Tai et al. 2015](https://aclanthology.info/papers/P15-1150/p15-1150) sought to address this with TreeLSTMs, which are fairly easy to implement in PyTorch.\n",
    "\n",
    "1. In the [distributed representations as features](#Distributed-representations-as-features) section, we just summed  all of the leaf-node GloVe vectors to obtain a fixed-dimensional representation for all sentences. This ignores all of the tree structure. See if you can do better by paying attention to the binary tree structure: write a function `glove_subtree_phi` that obtains a vector representation for each subtree by combining the vectors of its daughters, with the leaf nodes again given by GloVe (any dimension you like) and the full representation of the sentence given by the final vector obtained by this recursive process. You can decide on how you combine the vectors. \n",
    "\n",
    "1. If you have a lot of computing resources, then you can fire off a large hyperparameter search over many parameter values. All the model classes for this course are compatible with the `scikit-learn` and [scikit-optimize](https://scikit-optimize.github.io) methods, because they define the required functions for getting and setting parameters.\n",
    "\n",
    "We want to emphasize that this needs to be an __original__ system. It doesn't suffice to download code from the Web, retrain, and submit. You can build on others' code, but you have to do something new and meaningful with it.\n",
    "\n",
    "__Please include a brief prose description of your system along with your code, to help the teaching team understand the structure of your system.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "We tried various approaches for our own system, attempting to utilize the TorchShallowNeuralClassifier, RandomForestClassifier, and our custom 4-layer neural network (TorchCustomNeuralClassifier) before settling on using a TorchRNNClassifierModel. We extended the class to be able to handle multiple layers and utilize dropout.\n",
    "\n",
    "Our final system was a RNN that:\n",
    "\n",
    "1. Utilized contextual information using BERT uncased_L-12_H-768_A-12 model, with words as features.\n",
    "2. Explored the hyperparameter space. Used various values for hidden_dim, dropout, and bidirectionality.\n",
    "3. Tried using both 1 and 2 LSTM layers. However, we did not edit the classifer portion of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient \n",
    "\n",
    "def bert_sentence_phi(tree):\n",
    "    s = \" \".join(tree.leaves())\n",
    "    return bert_lookup[s]\n",
    "\n",
    "def bert_rnn_sentence_phi(tree):\n",
    "    s = \" \".join(tree.leaves())\n",
    "    return bert_word_lookup[s]\n",
    "\n",
    "def bert_reduce_mean(X):\n",
    "    return X.mean(axis=1)  \n",
    "\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "# Read train and dev\n",
    "sst_train_reader = sst.train_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_train = [(\" \".join(t.leaves()), label) for t, label in sst_train_reader]\n",
    "\n",
    "sst_dev_reader = sst.dev_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_dev = [(\" \".join(t.leaves()), label) for t, label in sst_dev_reader]\n",
    "\n",
    "# Zip\n",
    "X_str_train, y_train = zip(*sst_train)\n",
    "X_str_dev, y_dev = zip(*sst_dev)\n",
    "\n",
    "# Process examples into tokens\n",
    "X_bert_train, bert_train_toks = bc.encode(list(X_str_train), show_tokens=True)\n",
    "X_bert_dev, bert_dev_toks = bc.encode(list(X_str_dev), show_tokens=True)\n",
    "    \n",
    "# Reduce mean\n",
    "X_bert_train_mean = bert_reduce_mean(X_bert_train)\n",
    "X_bert_dev_mean = bert_reduce_mean(X_bert_dev)\n",
    "\n",
    "bert_lookup = {}\n",
    "for (sents, reps) in ((X_str_train, X_bert_train_mean), \n",
    "                      (X_str_dev, X_bert_dev_mean)):\n",
    "    assert len(sents) == len(reps)\n",
    "    for s, rep in zip(sents, reps):\n",
    "        bert_lookup[s] = rep\n",
    "        \n",
    "\n",
    "bert_word_lookup = {}\n",
    "for (sents, reps) in ((X_str_train, X_bert_train), \n",
    "                      (X_str_dev, X_bert_dev)):\n",
    "    assert len(sents) == len(reps)\n",
    "    for s, rep in zip(sents, reps):\n",
    "        bert_word_lookup[s] = rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "\n",
    "class TorchCustomNeuralClassifier(TorchShallowNeuralClassifier):\n",
    "    \"\"\"\n",
    "    Code based on TorchShallowNeuralClassifier.\n",
    "    \n",
    "    Fit a model\n",
    "\n",
    "    h = f(xW1 + b1)\n",
    "    y = softmax(hW2 + b2)\n",
    "\n",
    "    with a cross entropy loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_dim_1 : int\n",
    "        Dimensionality of the first hidden layer.\n",
    "    hidden_dim_2 : int\n",
    "        Dimensionality of the second hidden layer.\n",
    "    hidden_dim_3 : int\n",
    "        Dimensionality of the third hidden layer.\n",
    "    hidden_activation : vectorized activation function\n",
    "        The non-linear activation function used by the network for the\n",
    "        hidden layer. Default `nn.Tanh()`.\n",
    "    max_iter : int\n",
    "        Maximum number of training epochs.\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    optimizer : PyTorch optimizer\n",
    "        Default is `torch.optim.Adam`.\n",
    "    l2_strength : float\n",
    "        L2 regularization strength. Default 0 is no regularization.\n",
    "    device : 'cpu' or 'cuda'\n",
    "        The default is to use 'cuda' iff available\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TorchCustomNeuralClassifier, self).__init__(**kwargs)\n",
    "\n",
    "    def define_graph(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim_1),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim_1, self.hidden_dim_2),\n",
    "            self.hidden_activation,\n",
    "            torch.nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_dim_2, self.hidden_dim_3),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim_3, self.n_classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "from torch_rnn_classifier import TorchRNNClassifierModel\n",
    "\n",
    "class TorchMultilayerRNNClassifierModel(TorchRNNClassifierModel):\n",
    "    def __init__(self,\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            embedding,\n",
    "            use_embedding,\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            bidirectional,\n",
    "            device,\n",
    "            dropout,\n",
    "            num_layers):\n",
    "        super(TorchMultilayerRNNClassifierModel, self).__init__(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            embedding,\n",
    "            use_embedding,\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            bidirectional,\n",
    "            device)\n",
    "        # Graph\n",
    "        if self.use_embedding:\n",
    "            self.embedding = self._define_embedding(\n",
    "                embedding, vocab_size, self.embed_dim)\n",
    "            self.embed_dim = self.embedding.embedding_dim\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout)\n",
    "        if bidirectional:\n",
    "            classifier_dim = hidden_dim * 2\n",
    "        else:\n",
    "            classifier_dim = hidden_dim\n",
    "        self.classifier_layer = nn.Linear(classifier_dim, output_dim)\n",
    "\n",
    "\n",
    "class TorchMultilayerRNNClassifier(TorchRNNClassifier):\n",
    "    \"\"\"LSTM-based Recurrent Neural Network for classification problems.\n",
    "    The network will work for any kind of classification task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : list of str\n",
    "        This should be the vocabulary. It needs to be aligned with\n",
    "         `embedding` in the sense that the ith element of vocab\n",
    "        should be represented by the ith row of `embedding`. Ignored\n",
    "        if `use_embedding=False`.\n",
    "    embedding : np.array or None\n",
    "        Each row represents a word in `vocab`, as described above.\n",
    "    use_embedding : bool\n",
    "        If True, then incoming examples are presumed to be lists of\n",
    "        elements of the vocabulary. If False, then they are presumed\n",
    "        to be lists of vectors. In this case, the `embedding` and\n",
    "        `embed_dim` arguments are ignored, since no embedding is needed\n",
    "        and `embed_dim` is set by the nature of the incoming vectors.\n",
    "    embed_dim : int\n",
    "        Dimensionality for the initial embeddings. This is ignored\n",
    "        if `embedding` is not None, as a specified value there\n",
    "        determines this value. Also ignored if `use_embedding=False`.\n",
    "    hidden_dim : int\n",
    "        Dimensionality of the hidden layer.\n",
    "    bidirectional : bool\n",
    "        If True, then the final hidden states from passes in both\n",
    "        directions are used.\n",
    "    hidden_activation : vectorized activation function\n",
    "        The non-linear activation function used by the network for the\n",
    "        hidden layer. Default `nn.Tanh()`.\n",
    "    max_iter : int\n",
    "        Maximum number of training epochs.\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    optimizer : PyTorch optimizer\n",
    "        Default is `torch.optim.Adam`.\n",
    "    l2_strength : float\n",
    "        L2 regularization strength. Default 0 is no regularization.\n",
    "    device : 'cpu' or 'cuda'\n",
    "        The default is to use 'cuda' iff available\n",
    "    dropout : float\n",
    "        The amount of dropout to be used in the LSTM. The default is 0.\n",
    "    num_layers : int\n",
    "        The number of layers in the LSTM. The default is 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            vocab=[],\n",
    "            embedding=None,\n",
    "            use_embedding=True,\n",
    "            embed_dim=50,\n",
    "            bidirectional=False,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            **kwargs):\n",
    "        super(TorchMultilayerRNNClassifier, self).__init__(\n",
    "            vocab,\n",
    "            embedding=embedding,\n",
    "            use_embedding=use_embedding,\n",
    "            embed_dim=embed_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build_graph(self):\n",
    "        return TorchMultilayerRNNClassifierModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.n_classes_,\n",
    "            bidirectional=self.bidirectional,\n",
    "            device=self.device,\n",
    "            dropout=self.dropout,\n",
    "            num_layers=self.num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 0.0040959785692393785/Users/chungchang/anaconda3/envs/nlu/lib/python3.7/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Finished epoch 50 of 50; error is 0.0034767751931212842"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'bidirectional': True, 'dropout': 0, 'hidden_dim': 100, 'max_iter': 50, 'num_layers': 2, 'use_embedding': False, 'vocab': []}\n",
      "Best score: 0.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.691     0.757     0.723       988\n",
      "     neutral      0.381     0.268     0.315       522\n",
      "    positive      0.763     0.807     0.785      1054\n",
      "\n",
      "   micro avg      0.678     0.678     0.678      2564\n",
      "   macro avg      0.612     0.611     0.607      2564\n",
      "weighted avg      0.658     0.678     0.665      2564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit_custom_torch_shallow_neural_classifier(X, y):\n",
    "    base_mod = TorchShallowNeuralClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim': [250, 275, 300, 325, 350], \n",
    "        'hidden_activation': [nn.Tanh(), nn.ReLU()],\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "def fit_custom_random_forest_classifier(X, y):\n",
    "    base_mod = RandomForestClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'max_depth': [50, 100, 150, 200], \n",
    "        'max_features': ['auto', None]\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "def fit_custom_torch_neural_classifier(X, y):\n",
    "    base_mod = TorchCustomNeuralClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim_1': [275, 300, 325], \n",
    "        'hidden_dim_2': [150, 175], \n",
    "        'hidden_dim_3': [100, 125], \n",
    "        'hidden_activation': [nn.Tanh()],\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "def fit_custom_rnn_classifier(X, y):\n",
    "    bert_rnn = TorchMultilayerRNNClassifier()\n",
    "\n",
    "    # Warning: takes a VERY long time to run!\n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'vocab': [[]],\n",
    "        'use_embedding': [False],\n",
    "        'max_iter': [50],\n",
    "        'bidirectional': [True, False], \n",
    "        'hidden_dim': [50, 100], \n",
    "        'num_layers': [1, 2],\n",
    "        'dropout': [0, 0.1]\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, bert_rnn, cv, param_grid)\n",
    "\n",
    "    return best_mod\n",
    "        \n",
    "_ = sst.experiment(\n",
    "    SST_HOME,\n",
    "    bert_rnn_sentence_phi,\n",
    "    fit_custom_rnn_classifier,\n",
    "    class_func=sst.ternary_class_func,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in pickle for later usage in bakeoff\n",
    "import pickle\n",
    "pickle.dump(_['model'], open('sst_best_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.705     0.710     0.708       428\n",
      "     neutral      0.331     0.227     0.269       229\n",
      "    positive      0.725     0.838     0.777       444\n",
      "\n",
      "   micro avg      0.661     0.661     0.661      1101\n",
      "   macro avg      0.587     0.592     0.585      1101\n",
      "weighted avg      0.636     0.661     0.645      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Test load\n",
    "loaded_model = pickle.load(open('sst_best_model.pkl', 'rb'))\n",
    "loaded_model_preds = loaded_model.predict(X_bert_dev)\n",
    "print(classification_report(y_dev, loaded_model_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "The bake-off will begin on April 22. The announcement will go out on Piazza. As we said above, the bake-off evaluation data is the official SST test set release. For this bake-off, you'll evaluate your original system from the above homework problem on the test set, using the ternary class problem. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187246\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on April 24. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 0.0034276168735232204"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.732     0.752     0.742       912\n",
      "     neutral      0.318     0.260     0.286       389\n",
      "    positive      0.791     0.831     0.810       909\n",
      "\n",
      "   micro avg      0.698     0.698     0.698      2210\n",
      "   macro avg      0.613     0.614     0.613      2210\n",
      "weighted avg      0.683     0.698     0.690      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n",
    "\n",
    "# This is a copy of some code above, but cleaned up and edited to handle the test set.\n",
    "from bert_serving.client import BertClient \n",
    "\n",
    "def bert_rnn_sentence_phi(tree):\n",
    "    s = \" \".join(tree.leaves())\n",
    "    return bert_word_lookup[s]\n",
    "\n",
    "def fit_custom_rnn_classifier(X, y):\n",
    "    # Same as above, but without hyperparameter exploration\n",
    "    bert_rnn = TorchMultilayerRNNClassifier(\n",
    "        vocab=[],\n",
    "        use_embedding=False,\n",
    "        max_iter=50,\n",
    "        bidirectional=True,\n",
    "        hidden_dim=100,\n",
    "        num_layers=2,\n",
    "        dropout=0\n",
    "    )\n",
    "    bert_rnn.fit(X, y)\n",
    "    return bert_rnn\n",
    "\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "# Read train and test\n",
    "sst_train_reader = sst.train_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_train = [(\" \".join(t.leaves()), label) for t, label in sst_train_reader]\n",
    "\n",
    "sst_test_reader = sst.test_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_test = [(\" \".join(t.leaves()), label) for t, label in sst_test_reader]\n",
    "\n",
    "# Zip\n",
    "X_str_train, y_train = zip(*sst_train)\n",
    "X_str_test, y_test = zip(*sst_test)\n",
    "\n",
    "# Process examples into tokens\n",
    "X_bert_train, bert_train_toks = bc.encode(list(X_str_train), show_tokens=True)\n",
    "X_bert_test, bert_test_toks = bc.encode(list(X_str_test), show_tokens=True)        \n",
    "\n",
    "bert_word_lookup = {}\n",
    "for (sents, reps) in ((X_str_train, X_bert_train), \n",
    "                      (X_str_test, X_bert_test)):\n",
    "    assert len(sents) == len(reps)\n",
    "    for s, rep in zip(sents, reps):\n",
    "        bert_word_lookup[s] = rep\n",
    "        \n",
    "# Added vectorize=False\n",
    "bakeoff_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    bert_rnn_sentence_phi,\n",
    "    fit_custom_rnn_classifier,\n",
    "    train_reader=sst.train_reader,\n",
    "    assess_reader=sst.test_reader,\n",
    "    class_func=sst.ternary_class_func,\n",
    "    vectorize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
