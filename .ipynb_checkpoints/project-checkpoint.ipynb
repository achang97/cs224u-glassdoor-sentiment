{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glassdoor Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'cs224u_files')\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch_rnn_classifier import TorchRNNClassifier\n",
    "from torch_tree_nn import TorchTreeNN\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import utils\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import helpers\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLASSDOOR_HOME = 'glassdoor-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These splits are all stratified. We create a separate splits for the following:\n",
    "\n",
    "1. rating_overall\n",
    "2. rating_balance\n",
    "3. rating_culture\n",
    "4. rating_career\n",
    "5. rating_comp\n",
    "6. rating_mgmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created splits for rating_overall\n",
      "Created splits for rating_balance\n",
      "Created splits for rating_culture\n",
      "Created splits for rating_career\n",
      "Created splits for rating_comp\n",
      "Created splits for rating_mgmt\n"
     ]
    }
   ],
   "source": [
    "overall_split = helpers.train_dev_test_split(GLASSDOOR_HOME, random_state=1)\n",
    "balance_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_balance', random_state=1)\n",
    "culture_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_culture', random_state=1)\n",
    "career_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_career', random_state=1)\n",
    "comp_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_comp', random_state=1)\n",
    "mgmt_split = helpers.train_dev_test_split(GLASSDOOR_HOME, output_var='rating_mgmt', random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can show that these splits are stratified by checking the distribution. We'll do so for overall_split. As we can see, the overall ratings have a heavy left skew, as most ratings are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_overall, X_dev_overall, X_test_overall, y_train_overall, y_dev_overall, y_test_overall = overall_split\n",
    "\n",
    "def plot_hist(y, title):\n",
    "    ax = y.plot.hist(subplots=True, sharex=True, sharey=True, bins=[1, 2, 3, 4, 5, 6], title=title, xticks=[1, 2, 3, 4, 5])\n",
    "\n",
    "plot_hist(y_train_overall, 'Train Ratings')\n",
    "plot_hist(y_dev_overall, 'Dev Ratings')\n",
    "plot_hist(y_test_overall, 'Test Ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "from nltk.tree import Tree\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import scipy.stats\n",
    "import utils\n",
    "import os\n",
    "\n",
    "def experiment(\n",
    "        phi,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_assess,\n",
    "        y_assess,\n",
    "        train_func,\n",
    "        dev_iter=1,\n",
    "        class_func=helpers.ternary_class_func,\n",
    "        score_func=utils.safe_macro_f1,\n",
    "        vectorize=True,\n",
    "        verbose=True,\n",
    "        random_state=None):\n",
    "    \"\"\"Generic experimental framework. Either assesses with a\n",
    "    random train/test split of `train_reader` or with `assess_reader` if\n",
    "    it is given.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with keys\n",
    "        'model': trained model\n",
    "        'phi': the function used for featurization\n",
    "        'train_dataset': a dataset as returned by `build_dataset`\n",
    "        'assess_dataset': a dataset as returned by `build_dataset`\n",
    "        'predictions': predictions on the assessment data\n",
    "        'metric': `score_func.__name__`\n",
    "        'score': the `score_func` score on the assessment data\n",
    "\n",
    "    \"\"\"\n",
    "    # Train dataset:\n",
    "    train = helpers.build_dataset(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        phi,\n",
    "        class_func,\n",
    "        vectorizer=None,\n",
    "        vectorize=vectorize)\n",
    "    # Manage the assessment set-up:\n",
    "    X_train = train['X']\n",
    "    y_train = train['y']\n",
    "    raw_train = train['raw_examples']\n",
    "\n",
    "    # Assessment dataset using the training vectorizer:\n",
    "    assess = helpers.build_dataset(\n",
    "        X_assess,\n",
    "        y_assess,\n",
    "        phi,\n",
    "        class_func,\n",
    "        vectorizer=train['vectorizer'],\n",
    "        vectorize=vectorize)\n",
    "    X_assess, y_assess = assess['X'], assess['y']\n",
    "    \n",
    "    # Train:\n",
    "    mods = []\n",
    "    all_predictions = []\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(dev_iter):\n",
    "        prev_mod = mods[-1] if i != 0 else None\n",
    "        mod = train_func(X_train, y_train, prev_mod)\n",
    "        predictions = mod.predict(X_assess)\n",
    "        score = score_func(y_assess, predictions)\n",
    "    \n",
    "        mods.append(mod)\n",
    "        all_predictions.append(predictions)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Report:\n",
    "        if verbose:\n",
    "            print(classification_report(y_assess, predictions, digits=3))\n",
    "\n",
    "            # Return the overall score and experimental info:    \n",
    "    return {\n",
    "        'model': mods[0] if len(mods) == 1 else mods,\n",
    "        'phi': phi,\n",
    "        'train_dataset': train,\n",
    "        'assess_dataset': assess,\n",
    "        'predictions': all_predictions[0] if len(all_predictions) == 1 else all_predictions,\n",
    "        'metric': score_func.__name__,\n",
    "        'score': scores[0] if len(scores) == 1 else scores}\n",
    "\n",
    "def compare_models(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_assess,\n",
    "        y_assess,\n",
    "        phi1,\n",
    "        train_func1,\n",
    "        phi2=None,\n",
    "        train_func2=None,\n",
    "        vectorize1=True,\n",
    "        vectorize2=True,\n",
    "        stats_test=scipy.stats.wilcoxon,\n",
    "        trials=10,\n",
    "        dev_iter=1,\n",
    "        class_func=helpers.ternary_class_func,\n",
    "        score_func=utils.safe_macro_f1):\n",
    "    \"\"\"Wrapper for comparing models. The parameters are like those of\n",
    "    `experiment`, with the same defaults, except\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (np.array, np.array, float)\n",
    "        The first two are the scores from each model (length `trials`),\n",
    "        and the third is the p-value returned by stats_test.\n",
    "\n",
    "    \"\"\"\n",
    "    if phi2 == None:\n",
    "        phi2 = phi1\n",
    "    if train_func2 == None:\n",
    "        train_func2 = train_func1\n",
    "        \n",
    "    experiments1 = [experiment(\n",
    "        phi1,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_assess,\n",
    "        y_assess,\n",
    "        train_func=train_func1,\n",
    "        class_func=class_func,\n",
    "        score_func=score_func,\n",
    "        vectorize=vectorize1,\n",
    "        dev_iter=dev_iter,\n",
    "        verbose=False) for _ in range(trials)]\n",
    "    experiments2 = [experiment(\n",
    "        phi2,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_assess,\n",
    "        y_assess,\n",
    "        train_func=train_func2,\n",
    "        class_func=class_func,\n",
    "        score_func=score_func,\n",
    "        vectorize=vectorize2,\n",
    "        dev_iter=dev_iter,\n",
    "        verbose=False)  for _ in range(trials)]\n",
    "    scores1 = np.array([max(d['score']) for d in experiments1])\n",
    "    scores2 = np.array([max(d['score']) for d in experiments2])\n",
    "    # stats_test returns (test_statistic, p-value). We keep just the p-value:\n",
    "    pval = stats_test(scores1, scores2)[1]\n",
    "    # Report:\n",
    "    print('Model 1 mean: %0.03f' % scores1.mean())\n",
    "    print('Model 2 mean: %0.03f' % scores2.mean())\n",
    "    print('p = %0.03f' % pval if pval >= 0.001 else 'p < 0.001')\n",
    "    # Return the scores for later analysis, and the p value:\n",
    "    return (scores1, scores2, pval)\n",
    "\n",
    "def get_ci(vals):\n",
    "    if len(set(vals)) == 1:\n",
    "        return (vals[0], vals[0])\n",
    "    loc = np.mean(vals)\n",
    "    scale = np.std(vals) / np.sqrt(len(vals))\n",
    "    return stats.t.interval(0.95, len(vals)-1, loc=loc, scale=scale)\n",
    "\n",
    "def plot_score(scores, score_func=\"Macro F1\"):\n",
    "    scores = pd.Series(scores)\n",
    "    \n",
    "    ax = scores.plot()\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    _ = ax.set_ylabel(score_func)\n",
    "    \n",
    "def illustrative_confusion_matrix(data):\n",
    "    classes = ['pos', 'neg', 'neutral']\n",
    "    ex = pd.DataFrame(\n",
    "        data,        \n",
    "        columns=classes,\n",
    "        index=classes)\n",
    "    ex.index.name = \"observed\"\n",
    "    return ex\n",
    "\n",
    "def accuracy(cm):\n",
    "    return cm.values.diagonal().sum() / cm.values.sum()\n",
    "\n",
    "def precision(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=0)\n",
    "\n",
    "def recall(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "def f_score(cm, beta):\n",
    "    p = precision(cm)\n",
    "    r = recall(cm)\n",
    "    return (beta**2 + 1) * ((p * r) / ((beta**2 * p) + r))\n",
    "\n",
    "def f1_score(cm):\n",
    "    return f_score(cm, beta=1.0)\n",
    "\n",
    "def macro_f_score(cm, beta):\n",
    "    return f_score(cm, beta).mean(skipna=False)\n",
    "\n",
    "def precision_recall_curve(y, probs):\n",
    "    \"\"\"`y` is a list of labels, and `probs` is a list of predicted\n",
    "    probabilities or predicted scores -- likely a column of the \n",
    "    output of `predict_proba` using an `sklearn` classifier.\n",
    "    \"\"\"\n",
    "    thresholds = sorted(set(probs))\n",
    "    data = []\n",
    "    for t in thresholds:\n",
    "        # Use `t` to create labels:\n",
    "        pred = [1 if p >= t else 0 for p in probs]\n",
    "        # Precision/recall analysis as usual, focused on\n",
    "        # the positive class:\n",
    "        cm = pd.DataFrame(metrics.confusion_matrix(y, pred))\n",
    "        prec = precision(cm)[1]\n",
    "        rec = recall(cm)[1]\n",
    "        data.append((t, prec, rec))\n",
    "    # For intuitive graphs, always include this end-point:\n",
    "    data.append((None, 1, 0))\n",
    "    return pd.DataFrame(\n",
    "        data, columns=['threshold', 'precision', 'recall'])  \n",
    "\n",
    "def plot_precision_recall_curve(prc):\n",
    "    ax1 = prc.plot.scatter(x='recall', y='precision', legend=False)\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    ax1.set_ylabel(\"precision\")\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.set_xticklabels(prc['threshold'].values[::100].round(3))\n",
    "    _ = ax2.set_xlabel(\"threshold\")\n",
    "    \n",
    "def average_precision(p, r):\n",
    "    total = 0.0\n",
    "    for i in range(len(p)-1):\n",
    "        total += (r[i] - r[i+1]) * p[i]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A softmax baseline\n",
    "\n",
    "We first utilize a baseline approach that leverages a bag of words featurizer and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "def unigrams_phi(example):\n",
    "    counter = Counter()\n",
    "    \n",
    "    parts = example[['review_title', 'pros', 'cons', 'advice_to_mgmt']]\n",
    "    for part in parts:\n",
    "        counter.update(word_tokenize(part))\n",
    "        \n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def fit_softmax_classifier(X, y, prev_mod):   \n",
    "    if prev_mod is None:\n",
    "        mod = SGDClassifier(\n",
    "            loss='log',\n",
    "            fit_intercept=True)\n",
    "        new_mod = mod.partial_fit(X, y, classes=np.unique(y))\n",
    "    else:\n",
    "        new_mod = prev_mod.partial_fit(X, y)\n",
    "        \n",
    "    return new_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.656     0.332     0.441      1378\n",
      "     neutral      0.382     0.355     0.368      1980\n",
      "    positive      0.748     0.862     0.801      5396\n",
      "\n",
      "   micro avg      0.664     0.664     0.664      8754\n",
      "   macro avg      0.595     0.516     0.537      8754\n",
      "weighted avg      0.651     0.664     0.646      8754\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.567     0.522     0.544      1378\n",
      "     neutral      0.413     0.181     0.252      1980\n",
      "    positive      0.738     0.905     0.813      5396\n",
      "\n",
      "   micro avg      0.681     0.681     0.681      8754\n",
      "   macro avg      0.573     0.536     0.536      8754\n",
      "weighted avg      0.638     0.681     0.644      8754\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.556     0.482     0.516      1378\n",
      "     neutral      0.346     0.580     0.434      1980\n",
      "    positive      0.843     0.663     0.742      5396\n",
      "\n",
      "   micro avg      0.616     0.616     0.616      8754\n",
      "   macro avg      0.582     0.575     0.564      8754\n",
      "weighted avg      0.685     0.616     0.637      8754\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.518     0.594     0.553      1378\n",
      "     neutral      0.390     0.322     0.352      1980\n",
      "    positive      0.794     0.815     0.804      5396\n",
      "\n",
      "   micro avg      0.668     0.668     0.668      8754\n",
      "   macro avg      0.567     0.577     0.570      8754\n",
      "weighted avg      0.659     0.668     0.662      8754\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.586     0.374     0.456      1378\n",
      "     neutral      0.359     0.274     0.311      1980\n",
      "    positive      0.731     0.862     0.791      5396\n",
      "\n",
      "   micro avg      0.652     0.652     0.652      8754\n",
      "   macro avg      0.559     0.503     0.519      8754\n",
      "weighted avg      0.624     0.652     0.630      8754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_experiment = experiment(\n",
    "    unigrams_phi,\n",
    "    X_train_overall,\n",
    "    y_train_overall,\n",
    "    X_dev_overall,\n",
    "    y_dev_overall,\n",
    "    fit_softmax_classifier,\n",
    "    dev_iter=5,\n",
    "    class_func=helpers.ternary_class_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNClassifier wrapper\n",
    "\n",
    "This section illustrates how to use `sst.experiment` with RNN and TreeNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To featurize examples for an RNN, we just get the words in order, letting the model take care of mapping them into an embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_phi(tree):\n",
    "    return tree.leaves()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model wrapper gets the vocabulary using `sst.get_vocab`. If you want to use pretrained word representations in here, then you can have `fit_rnn_classifier` build that space too; see [this notebook section for details](sst_03_neural_networks.ipynb#Pretrained-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn_classifier(X, y):    \n",
    "    sst_glove_vocab = utils.get_vocab(X, n_words=10000)     \n",
    "    mod = TorchRNNClassifier(\n",
    "        sst_glove_vocab, \n",
    "        eta=0.05,\n",
    "        embedding=None,\n",
    "        batch_size=1000,\n",
    "        embed_dim=50,\n",
    "        hidden_dim=50,\n",
    "        max_iter=50,\n",
    "        l2_strength=0.001,\n",
    "        bidirectional=True,\n",
    "        hidden_activation=nn.ReLU())\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 2.2390241771936417"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.601     0.661     0.630       428\n",
      "     neutral      0.274     0.227     0.248       229\n",
      "    positive      0.630     0.624     0.627       444\n",
      "\n",
      "   micro avg      0.556     0.556     0.556      1101\n",
      "   macro avg      0.501     0.504     0.501      1101\n",
      "weighted avg      0.544     0.556     0.549      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    rnn_phi,\n",
    "    fit_rnn_classifier, \n",
    "    vectorize=False,  # For deep learning, use `vectorize=False`.\n",
    "    assess_reader=sst.dev_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "This section begins to build an error-analysis framework using the dicts returned by `sst.experiment`. These have the following structure:\n",
    "\n",
    "```\n",
    "'model': trained model\n",
    "'train_dataset':\n",
    "   'X': feature matrix\n",
    "   'y': list of labels\n",
    "   'vectorizer': DictVectorizer,\n",
    "   'raw_examples': list of raw inputs, before featurizing   \n",
    "'assess_dataset': same structure as the value of 'train_dataset'\n",
    "'predictions': predictions on the assessment data\n",
    "'metric': `score_func.__name__`, where `score_func` is an `sst.experiment` argument\n",
    "'score': the `score_func` score on the assessment data\n",
    "```\n",
    "The following function just finds mistakes, and returns a `pd.DataFrame` for easy subsequent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(experiment):\n",
    "    \"\"\"Find mistaken predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    experiment : dict\n",
    "        As returned by `sst.experiment`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    raw_examples = experiment['assess_dataset']['raw_examples']\n",
    "    raw_examples = [\" \".join(tree.leaves()) for tree in raw_examples]\n",
    "    df = pd.DataFrame({\n",
    "        'raw_examples': raw_examples,\n",
    "        'predicted': experiment['predictions'],\n",
    "        'gold': experiment['assess_dataset']['y']})\n",
    "    df['correct'] = df['predicted'] == df['gold']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_analysis = find_errors(softmax_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_analysis = find_errors(rnn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merge the sotmax and RNN experiments into a single DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = softmax_analysis.merge(\n",
    "    rnn_analysis, left_on='raw_examples', right_on='raw_examples')\n",
    "\n",
    "analysis = analysis.drop('gold_y', axis=1).rename(columns={'gold_x': 'gold'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code collects a specific subset of examples; small modifications to its structure will give you different interesting subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where the softmax model is correct, the RNN is not,\n",
    "# and the gold label is 'positive'\n",
    "\n",
    "error_group = analysis[\n",
    "    (analysis['predicted_x'] == analysis['gold'])\n",
    "    &\n",
    "    (analysis['predicted_y'] != analysis['gold'])    \n",
    "    &\n",
    "    (analysis['gold'] == 'positive')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_group.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "It 's a beautiful madness .\n",
      "======================================================================\n",
      "One of the smartest takes on singles culture I 've seen in a long time .\n",
      "======================================================================\n",
      "But it still jingles in the pocket .\n",
      "======================================================================\n",
      "For the first time in years , De Niro digs deep emotionally , perhaps because he 's been stirred by the powerful work of his co-stars .\n",
      "======================================================================\n",
      "With Rabbit-Proof Fence , Noyce has tailored an epic tale into a lean , economical movie .\n"
     ]
    }
   ],
   "source": [
    "for ex in error_group['raw_examples'].sample(5):\n",
    "    print(\"=\"*70)\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "\n",
    "Please embed your homework responses in this notebook, and do not delete any cells from the notebook. (You are free to add as many cells as you like as part of your responses.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment words alone [2 points]\n",
    "\n",
    "NLTK includes an easy interface to [Minqing Hu and Bing Liu's __Opinion Lexicon__](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which consists of a list of positive words and a list of negative words. How much of the ternary SST story does this lexicon tell?\n",
    "\n",
    "For this problem, submit code to do the following:\n",
    "\n",
    "1. Create a feature function `op_unigrams` on the model of `unigrams_phi` above, but filtering the vocabulary to just items that are members of the Opinion Lexicon. Submit this feature function.\n",
    "\n",
    "1. Evaluate your feature function with `sst.experiment`, with all the same parameters as were used to create `softmax_experiment` in [A softmax baseline](#A-softmax-baseline) above, except of course for the feature function.\n",
    "\n",
    "1. Use `utils.mcnemar` to compare your feature function with the results in `softmax_experiment`. The information you need for this is in `softmax_experiment` and your own `sst.experiment` results. Submit your evaluation code. You can assume `softmax_experiment` is already in memory, but your code should create the other objects necessary for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.553     0.752     0.638       428\n",
      "     neutral      0.179     0.031     0.052       229\n",
      "    positive      0.615     0.664     0.639       444\n",
      "\n",
      "   micro avg      0.567     0.567     0.567      1101\n",
      "   macro avg      0.449     0.482     0.443      1101\n",
      "weighted avg      0.500     0.567     0.516      1101\n",
      "\n",
      "McNemar's test: 5.33 (0.020980477345314247)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Use set for fast membership checking:\n",
    "positive = set(opinion_lexicon.positive())\n",
    "negative = set(opinion_lexicon.negative())\n",
    "\n",
    "def op_unigrams(tree):\n",
    "    filtered_unigrams = [word for word in tree.leaves() if word in positive or word in negative]\n",
    "    return Counter(filtered_unigrams)\n",
    "\n",
    "softmax_op_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    op_unigrams,                      # Free to write your own!\n",
    "    fit_softmax_classifier,            # Free to write your own!\n",
    "    train_reader=sst.train_reader,     # Fixed by the competition.\n",
    "    assess_reader=sst.dev_reader,      # Fixed until the bake-off.\n",
    "    class_func=sst.ternary_class_func) # Fixed by the bake-off rules.\n",
    "\n",
    "m = utils.mcnemar(\n",
    "    softmax_experiment['assess_dataset']['y'], \n",
    "    softmax_experiment['predictions'],\n",
    "    softmax_op_experiment['predictions'])\n",
    "\n",
    "p = \"p < 0.0001\" if m[1] < 0.0001 else m[1]\n",
    "print(\"McNemar's test: {0:0.02f} ({1:})\".format(m[0], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more powerful vector-summing baseline [3 points]\n",
    "\n",
    "In [Distributed representations as features](sst_03_neural_networks.ipynb#Distributed-representations-as-features), we looked at a baseline for the ternary SST problem in which each example is modeled as the sum of its 50-dimensional GloVe representations. A `LogisticRegression` model was used for prediction. A neural network might do better here, since there might be complex relationships between the input feature dimensions that a linear classifier can't learn. \n",
    "\n",
    "To address this question, rerun the experiment with `torch_shallow_neural_classifier.TorchShallowNeuralClassifier` as the classifier. Specs:\n",
    "* Use `sst.experiment` to conduct the experiment. \n",
    "* Using 3-fold cross-validation, exhaustively explore this set of hyperparameter combinations:\n",
    "  * The hidden dimensionality at 50, 100, and 200.\n",
    "  * The hidden activation function as `nn.Tanh` or `nn.ReLU`.\n",
    "* (For all other parameters to `TorchShallowNeuralClassifier`, use the defaults.)\n",
    "\n",
    "For this problem, submit code to do the following:\n",
    "\n",
    "1. Your model wrapper function around `TorchShallowNeuralClassifier`. This function should implement the requisite cross-validation; see [this notebook section](sst_02_hand_built_features.ipynb#Hyperparameter-search) for examples.\n",
    "1. Your average F1 score according to `sst.experiment`. \n",
    "2. The optimal hyperparameters chosen in your experiment. (You can just paste in the dict that `sst._experiment` prints.)\n",
    "\n",
    "We're not evaluating the quality of your model. (We've specified the protocols completely, but there will still be a  lot of variation in the results.) However, the primary goal of this question is to get you thinking more about this strikingly good baseline feature representation scheme for SST, so we're sort of hoping you feel compelled to try out variations on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
    "\n",
    "def vsm_leaves_phi(tree, lookup, np_func=np.sum):\n",
    "    \"\"\"Represent `tree` as a combination of the vector of its words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : nltk.Tree   \n",
    "    lookup : dict\n",
    "        From words to vectors.\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. The requirement is that \n",
    "        the function take `axis=0` as one of its arguments (to ensure\n",
    "        columnwise combination) and that it return a vector of a \n",
    "        fixed length, no matter what the size of the tree is.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `X.shape[1]`\n",
    "            \n",
    "    \"\"\"      \n",
    "    allvecs = np.array([lookup[w] for w in tree.leaves() if w in lookup])    \n",
    "    if len(allvecs) == 0:\n",
    "        dim = len(next(iter(lookup.values())))\n",
    "        feats = np.zeros(dim)\n",
    "    else:       \n",
    "        feats = np_func(allvecs, axis=0)      \n",
    "    return feats\n",
    "\n",
    "def glove_leaves_phi(tree, np_func=np.sum):\n",
    "    return vsm_leaves_phi(tree, glove_lookup, np_func=np_func)\n",
    "\n",
    "def fit_torch_shallow_neural_classifier(X, y):\n",
    "    base_mod = TorchShallowNeuralClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim': [50, 100, 200], \n",
    "        'hidden_activation': [nn.Tanh(), nn.ReLU()],\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "DATE_HOME = 'data'\n",
    "GLOVE_HOME = os.path.join(DATE_HOME, 'glove.6B')\n",
    "\n",
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(GLOVE_HOME, 'glove.6B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 100 of 100; error is 2.30353552103042693"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'hidden_activation': Tanh(), 'hidden_dim': 50}\n",
      "Best score: 0.520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.621     0.645     0.633       989\n",
      "     neutral      0.304     0.191     0.235       498\n",
      "    positive      0.647     0.735     0.688      1077\n",
      "\n",
      "   micro avg      0.595     0.595     0.595      2564\n",
      "   macro avg      0.524     0.524     0.519      2564\n",
      "weighted avg      0.570     0.595     0.579      2564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = sst.experiment(\n",
    "    SST_HOME,\n",
    "    glove_leaves_phi,\n",
    "    fit_torch_shallow_neural_classifier,\n",
    "    class_func=sst.ternary_class_func,\n",
    "    vectorize=False)  # Tell `experiment` that we already have our feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The macro-averaged F1 score was 0.519 and the best parameters were {'hidden_activation': Tanh(), 'hidden_dim': 50}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your original system [4 points]\n",
    "\n",
    "Your task is to develop an original model for the SST ternary problem. There are many options. If you spend more than a few hours on this homework problem, you should consider letting it grow into your final project! Here are some relatively manageable ideas that you might try:\n",
    "\n",
    "1. We didn't systematically evaluate the `bidirectional` option to the `TorchRNNClassifier`. Similarly, that model could be tweaked to allow multiple LSTM layers (at present there is only one), and you could try adding layers to the classifier portion of the model as well.\n",
    "\n",
    "1. We've already glimpsed the power of rich initial word representations, and later in the course we'll see that smart initialization usually leads to a performance gain in NLP, so you could perhaps achieve a winning entry with a simple model that starts in a great place.\n",
    "\n",
    "1. The [practical introduction to contextual word representations](contextualreps.ipynb) (to be discussed later in the quarter) covers pretrained representations and interfaces that are likely to boost the performance of any system.\n",
    "\n",
    "1. The `TreeNN` and `TorchTreeNN` don't perform all that well, and this could be for the same reason that RNNs don't peform well: the gradient signal doesn't propagate reliably down inside very deep trees. [Tai et al. 2015](https://aclanthology.info/papers/P15-1150/p15-1150) sought to address this with TreeLSTMs, which are fairly easy to implement in PyTorch.\n",
    "\n",
    "1. In the [distributed representations as features](#Distributed-representations-as-features) section, we just summed  all of the leaf-node GloVe vectors to obtain a fixed-dimensional representation for all sentences. This ignores all of the tree structure. See if you can do better by paying attention to the binary tree structure: write a function `glove_subtree_phi` that obtains a vector representation for each subtree by combining the vectors of its daughters, with the leaf nodes again given by GloVe (any dimension you like) and the full representation of the sentence given by the final vector obtained by this recursive process. You can decide on how you combine the vectors. \n",
    "\n",
    "1. If you have a lot of computing resources, then you can fire off a large hyperparameter search over many parameter values. All the model classes for this course are compatible with the `scikit-learn` and [scikit-optimize](https://scikit-optimize.github.io) methods, because they define the required functions for getting and setting parameters.\n",
    "\n",
    "We want to emphasize that this needs to be an __original__ system. It doesn't suffice to download code from the Web, retrain, and submit. You can build on others' code, but you have to do something new and meaningful with it.\n",
    "\n",
    "__Please include a brief prose description of your system along with your code, to help the teaching team understand the structure of your system.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description\n",
    "\n",
    "We tried various approaches for our own system, attempting to utilize the TorchShallowNeuralClassifier, RandomForestClassifier, and our custom 4-layer neural network (TorchCustomNeuralClassifier) before settling on using a TorchRNNClassifierModel. We extended the class to be able to handle multiple layers and utilize dropout.\n",
    "\n",
    "Our final system was a RNN that:\n",
    "\n",
    "1. Utilized contextual information using BERT uncased_L-12_H-768_A-12 model, with words as features.\n",
    "2. Explored the hyperparameter space. Used various values for hidden_dim, dropout, and bidirectionality.\n",
    "3. Tried using both 1 and 2 LSTM layers. However, we did not edit the classifer portion of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient \n",
    "\n",
    "def bert_sentence_phi(tree):\n",
    "    s = \" \".join(tree.leaves())\n",
    "    return bert_lookup[s]\n",
    "\n",
    "def bert_rnn_sentence_phi(tree):\n",
    "    s = \" \".join(tree.leaves())\n",
    "    return bert_word_lookup[s]\n",
    "\n",
    "def bert_reduce_mean(X):\n",
    "    return X.mean(axis=1)  \n",
    "\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "# Read train and dev\n",
    "sst_train_reader = sst.train_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_train = [(\" \".join(t.leaves()), label) for t, label in sst_train_reader]\n",
    "\n",
    "sst_dev_reader = sst.dev_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_dev = [(\" \".join(t.leaves()), label) for t, label in sst_dev_reader]\n",
    "\n",
    "# Zip\n",
    "X_str_train, y_train = zip(*sst_train)\n",
    "X_str_dev, y_dev = zip(*sst_dev)\n",
    "\n",
    "# Process examples into tokens\n",
    "X_bert_train, bert_train_toks = bc.encode(list(X_str_train), show_tokens=True)\n",
    "X_bert_dev, bert_dev_toks = bc.encode(list(X_str_dev), show_tokens=True)\n",
    "    \n",
    "# Reduce mean\n",
    "X_bert_train_mean = bert_reduce_mean(X_bert_train)\n",
    "X_bert_dev_mean = bert_reduce_mean(X_bert_dev)\n",
    "\n",
    "bert_lookup = {}\n",
    "for (sents, reps) in ((X_str_train, X_bert_train_mean), \n",
    "                      (X_str_dev, X_bert_dev_mean)):\n",
    "    assert len(sents) == len(reps)\n",
    "    for s, rep in zip(sents, reps):\n",
    "        bert_lookup[s] = rep\n",
    "        \n",
    "\n",
    "bert_word_lookup = {}\n",
    "for (sents, reps) in ((X_str_train, X_bert_train), \n",
    "                      (X_str_dev, X_bert_dev)):\n",
    "    assert len(sents) == len(reps)\n",
    "    for s, rep in zip(sents, reps):\n",
    "        bert_word_lookup[s] = rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "\n",
    "class TorchCustomNeuralClassifier(TorchShallowNeuralClassifier):\n",
    "    \"\"\"\n",
    "    Code based on TorchShallowNeuralClassifier.\n",
    "    \n",
    "    Fit a model\n",
    "\n",
    "    h = f(xW1 + b1)\n",
    "    y = softmax(hW2 + b2)\n",
    "\n",
    "    with a cross entropy loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_dim_1 : int\n",
    "        Dimensionality of the first hidden layer.\n",
    "    hidden_dim_2 : int\n",
    "        Dimensionality of the second hidden layer.\n",
    "    hidden_dim_3 : int\n",
    "        Dimensionality of the third hidden layer.\n",
    "    hidden_activation : vectorized activation function\n",
    "        The non-linear activation function used by the network for the\n",
    "        hidden layer. Default `nn.Tanh()`.\n",
    "    max_iter : int\n",
    "        Maximum number of training epochs.\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    optimizer : PyTorch optimizer\n",
    "        Default is `torch.optim.Adam`.\n",
    "    l2_strength : float\n",
    "        L2 regularization strength. Default 0 is no regularization.\n",
    "    device : 'cpu' or 'cuda'\n",
    "        The default is to use 'cuda' iff available\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TorchCustomNeuralClassifier, self).__init__(**kwargs)\n",
    "\n",
    "    def define_graph(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim_1),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim_1, self.hidden_dim_2),\n",
    "            self.hidden_activation,\n",
    "            torch.nn.Dropout(0.1),\n",
    "            nn.Linear(self.hidden_dim_2, self.hidden_dim_3),\n",
    "            self.hidden_activation,\n",
    "            nn.Linear(self.hidden_dim_3, self.n_classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from torch_model_base import TorchModelBase\n",
    "from utils import progress_bar\n",
    "from torch_rnn_classifier import TorchRNNClassifierModel\n",
    "\n",
    "class TorchMultilayerRNNClassifierModel(TorchRNNClassifierModel):\n",
    "    def __init__(self,\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            embedding,\n",
    "            use_embedding,\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            bidirectional,\n",
    "            device,\n",
    "            dropout,\n",
    "            num_layers):\n",
    "        super(TorchMultilayerRNNClassifierModel, self).__init__(\n",
    "            vocab_size,\n",
    "            embed_dim,\n",
    "            embedding,\n",
    "            use_embedding,\n",
    "            hidden_dim,\n",
    "            output_dim,\n",
    "            bidirectional,\n",
    "            device)\n",
    "        # Graph\n",
    "        if self.use_embedding:\n",
    "            self.embedding = self._define_embedding(\n",
    "                embedding, vocab_size, self.embed_dim)\n",
    "            self.embed_dim = self.embedding.embedding_dim\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout)\n",
    "        if bidirectional:\n",
    "            classifier_dim = hidden_dim * 2\n",
    "        else:\n",
    "            classifier_dim = hidden_dim\n",
    "        self.classifier_layer = nn.Linear(classifier_dim, output_dim)\n",
    "\n",
    "\n",
    "class TorchMultilayerRNNClassifier(TorchRNNClassifier):\n",
    "    \"\"\"LSTM-based Recurrent Neural Network for classification problems.\n",
    "    The network will work for any kind of classification task.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : list of str\n",
    "        This should be the vocabulary. It needs to be aligned with\n",
    "         `embedding` in the sense that the ith element of vocab\n",
    "        should be represented by the ith row of `embedding`. Ignored\n",
    "        if `use_embedding=False`.\n",
    "    embedding : np.array or None\n",
    "        Each row represents a word in `vocab`, as described above.\n",
    "    use_embedding : bool\n",
    "        If True, then incoming examples are presumed to be lists of\n",
    "        elements of the vocabulary. If False, then they are presumed\n",
    "        to be lists of vectors. In this case, the `embedding` and\n",
    "        `embed_dim` arguments are ignored, since no embedding is needed\n",
    "        and `embed_dim` is set by the nature of the incoming vectors.\n",
    "    embed_dim : int\n",
    "        Dimensionality for the initial embeddings. This is ignored\n",
    "        if `embedding` is not None, as a specified value there\n",
    "        determines this value. Also ignored if `use_embedding=False`.\n",
    "    hidden_dim : int\n",
    "        Dimensionality of the hidden layer.\n",
    "    bidirectional : bool\n",
    "        If True, then the final hidden states from passes in both\n",
    "        directions are used.\n",
    "    hidden_activation : vectorized activation function\n",
    "        The non-linear activation function used by the network for the\n",
    "        hidden layer. Default `nn.Tanh()`.\n",
    "    max_iter : int\n",
    "        Maximum number of training epochs.\n",
    "    eta : float\n",
    "        Learning rate.\n",
    "    optimizer : PyTorch optimizer\n",
    "        Default is `torch.optim.Adam`.\n",
    "    l2_strength : float\n",
    "        L2 regularization strength. Default 0 is no regularization.\n",
    "    device : 'cpu' or 'cuda'\n",
    "        The default is to use 'cuda' iff available\n",
    "    dropout : float\n",
    "        The amount of dropout to be used in the LSTM. The default is 0.\n",
    "    num_layers : int\n",
    "        The number of layers in the LSTM. The default is 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "            vocab=[],\n",
    "            embedding=None,\n",
    "            use_embedding=True,\n",
    "            embed_dim=50,\n",
    "            bidirectional=False,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            **kwargs):\n",
    "        super(TorchMultilayerRNNClassifier, self).__init__(\n",
    "            vocab,\n",
    "            embedding=embedding,\n",
    "            use_embedding=use_embedding,\n",
    "            embed_dim=embed_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            **kwargs)\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def build_graph(self):\n",
    "        return TorchMultilayerRNNClassifierModel(\n",
    "            vocab_size=len(self.vocab),\n",
    "            embedding=self.embedding,\n",
    "            use_embedding=self.use_embedding,\n",
    "            embed_dim=self.embed_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.n_classes_,\n",
    "            bidirectional=self.bidirectional,\n",
    "            device=self.device,\n",
    "            dropout=self.dropout,\n",
    "            num_layers=self.num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 0.0040959785692393785/Users/chungchang/anaconda3/envs/nlu/lib/python3.7/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Finished epoch 50 of 50; error is 0.0034767751931212842"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'bidirectional': True, 'dropout': 0, 'hidden_dim': 100, 'max_iter': 50, 'num_layers': 2, 'use_embedding': False, 'vocab': []}\n",
      "Best score: 0.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.691     0.757     0.723       988\n",
      "     neutral      0.381     0.268     0.315       522\n",
      "    positive      0.763     0.807     0.785      1054\n",
      "\n",
      "   micro avg      0.678     0.678     0.678      2564\n",
      "   macro avg      0.612     0.611     0.607      2564\n",
      "weighted avg      0.658     0.678     0.665      2564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def fit_custom_torch_shallow_neural_classifier(X, y):\n",
    "    base_mod = TorchShallowNeuralClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim': [250, 275, 300, 325, 350], \n",
    "        'hidden_activation': [nn.Tanh(), nn.ReLU()],\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "def fit_custom_random_forest_classifier(X, y):\n",
    "    base_mod = RandomForestClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'max_depth': [50, 100, 150, 200], \n",
    "        'max_features': ['auto', None]\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "def fit_custom_torch_neural_classifier(X, y):\n",
    "    base_mod = TorchCustomNeuralClassifier()\n",
    "    \n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'hidden_dim_1': [275, 300, 325], \n",
    "        'hidden_dim_2': [150, 175], \n",
    "        'hidden_dim_3': [100, 125], \n",
    "        'hidden_activation': [nn.Tanh()],\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, base_mod, cv, param_grid)\n",
    "    \n",
    "    return best_mod\n",
    "\n",
    "def fit_custom_rnn_classifier(X, y):\n",
    "    bert_rnn = TorchMultilayerRNNClassifier()\n",
    "\n",
    "    # Warning: takes a VERY long time to run!\n",
    "    cv = 3\n",
    "    param_grid = {\n",
    "        'vocab': [[]],\n",
    "        'use_embedding': [False],\n",
    "        'max_iter': [50],\n",
    "        'bidirectional': [True, False], \n",
    "        'hidden_dim': [50, 100], \n",
    "        'num_layers': [1, 2],\n",
    "        'dropout': [0, 0.1]\n",
    "    }   \n",
    "\n",
    "    best_mod = utils.fit_classifier_with_crossvalidation(\n",
    "        X, y, bert_rnn, cv, param_grid)\n",
    "\n",
    "    return best_mod\n",
    "        \n",
    "_ = sst.experiment(\n",
    "    SST_HOME,\n",
    "    bert_rnn_sentence_phi,\n",
    "    fit_custom_rnn_classifier,\n",
    "    class_func=sst.ternary_class_func,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model in pickle for later usage in bakeoff\n",
    "import pickle\n",
    "pickle.dump(_['model'], open('sst_best_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.705     0.710     0.708       428\n",
      "     neutral      0.331     0.227     0.269       229\n",
      "    positive      0.725     0.838     0.777       444\n",
      "\n",
      "   micro avg      0.661     0.661     0.661      1101\n",
      "   macro avg      0.587     0.592     0.585      1101\n",
      "weighted avg      0.636     0.661     0.645      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Test load\n",
    "loaded_model = pickle.load(open('sst_best_model.pkl', 'rb'))\n",
    "loaded_model_preds = loaded_model.predict(X_bert_dev)\n",
    "print(classification_report(y_dev, loaded_model_preds, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off [1 point]\n",
    "\n",
    "The bake-off will begin on April 22. The announcement will go out on Piazza. As we said above, the bake-off evaluation data is the official SST test set release. For this bake-off, you'll evaluate your original system from the above homework problem on the test set, using the ternary class problem. Rules:\n",
    "\n",
    "1. Only one evaluation is permitted.\n",
    "1. No additional system tuning is permitted once the bake-off has started.\n",
    "\n",
    "To enter the bake-off, upload this notebook on Canvas:\n",
    "\n",
    "https://canvas.stanford.edu/courses/99711/assignments/187246\n",
    "\n",
    "The cells below this one constitute your bake-off entry.\n",
    "\n",
    "Systems that enter will receive the additional homework point, and systems that achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
    "\n",
    "The bake-off will close at 4:30 pm on April 24. Late entries will be accepted, but they cannot earn the extra 0.5 points. Similarly, you cannot win the bake-off unless your homework is submitted on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished epoch 50 of 50; error is 0.0034276168735232204"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative      0.732     0.752     0.742       912\n",
      "     neutral      0.318     0.260     0.286       389\n",
      "    positive      0.791     0.831     0.810       909\n",
      "\n",
      "   micro avg      0.698     0.698     0.698      2210\n",
      "   macro avg      0.613     0.614     0.613      2210\n",
      "weighted avg      0.683     0.698     0.690      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enter your bake-off assessment code in this cell. \n",
    "# Please do not remove this comment.\n",
    "\n",
    "# This is a copy of some code above, but cleaned up and edited to handle the test set.\n",
    "from bert_serving.client import BertClient \n",
    "\n",
    "def bert_rnn_sentence_phi(tree):\n",
    "    s = \" \".join(tree.leaves())\n",
    "    return bert_word_lookup[s]\n",
    "\n",
    "def fit_custom_rnn_classifier(X, y):\n",
    "    # Same as above, but without hyperparameter exploration\n",
    "    bert_rnn = TorchMultilayerRNNClassifier(\n",
    "        vocab=[],\n",
    "        use_embedding=False,\n",
    "        max_iter=50,\n",
    "        bidirectional=True,\n",
    "        hidden_dim=100,\n",
    "        num_layers=2,\n",
    "        dropout=0\n",
    "    )\n",
    "    bert_rnn.fit(X, y)\n",
    "    return bert_rnn\n",
    "\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "# Read train and test\n",
    "sst_train_reader = sst.train_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_train = [(\" \".join(t.leaves()), label) for t, label in sst_train_reader]\n",
    "\n",
    "sst_test_reader = sst.test_reader(\n",
    "    SST_HOME, class_func=sst.ternary_class_func)\n",
    "sst_test = [(\" \".join(t.leaves()), label) for t, label in sst_test_reader]\n",
    "\n",
    "# Zip\n",
    "X_str_train, y_train = zip(*sst_train)\n",
    "X_str_test, y_test = zip(*sst_test)\n",
    "\n",
    "# Process examples into tokens\n",
    "X_bert_train, bert_train_toks = bc.encode(list(X_str_train), show_tokens=True)\n",
    "X_bert_test, bert_test_toks = bc.encode(list(X_str_test), show_tokens=True)        \n",
    "\n",
    "bert_word_lookup = {}\n",
    "for (sents, reps) in ((X_str_train, X_bert_train), \n",
    "                      (X_str_test, X_bert_test)):\n",
    "    assert len(sents) == len(reps)\n",
    "    for s, rep in zip(sents, reps):\n",
    "        bert_word_lookup[s] = rep\n",
    "        \n",
    "# Added vectorize=False\n",
    "bakeoff_experiment = sst.experiment(\n",
    "    SST_HOME,\n",
    "    bert_rnn_sentence_phi,\n",
    "    fit_custom_rnn_classifier,\n",
    "    train_reader=sst.train_reader,\n",
    "    assess_reader=sst.test_reader,\n",
    "    class_func=sst.ternary_class_func,\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On an otherwise blank line in this cell, please enter\n",
    "# your macro-average F1 value as reported by the code above. \n",
    "# Please enter only a number between 0 and 1 inclusive.\n",
    "# Please do not remove this comment.\n",
    "0.613"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
